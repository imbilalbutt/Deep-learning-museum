{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alternatvie text](https://www.doc.zuv.fau.de//M/FAU-Logo/01_FAU_Kernmarke/Web/FAU_Kernmarke_Q_RGB_blue.svg)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5: Few-Shot Learning "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture you have learned optimization-based meta-learning techniques with application to few-shot image classification and segmentation. In this notebook, you'll be implementing First-order MAML (FOMAML) and Reptile algorithms for few-shot image classification task. You'll use CIFAR-FS dataset throughout this exercise.\n",
    "\n",
    "Note: You are required to install [**torchmeta**](https://github.com/tristandeleu/pytorch-meta) to assist you in your assignment. Also, note that this library support PyTorch up to version 1.10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Task Generator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you'll be required to implement a task generator for few-shot classification using torchmeta. Subsquently, the generator will be used as argument in the Dataloader for training and testing with FOMAML and Reptile. Recall a few-shot classification task is formulated as *N*-way, *K*-shot problem, where the *NK* data samples form the support set. Set *N* = 5 and *K*=5. Also, set the size of the query set to 15. The below figure visulaizes a few-shot classification task."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"task.png\" width=\"250\" height=\"300\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Omniglot dataset contains handwritten characters from different alphabets, visualize some of the dataset samples below. You'll use the train set in meta-training and the test set for meta-testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchmeta.datasets.helpers import omniglot,cifar_fs\n",
    "from torchmeta.utils.data import BatchMetaDataLoader\n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) FOMAML\n",
    "FOMAML is less computationally expensive than MAML as it does not require second-order derivative computation. In this task you need to implement FOMAML algorithm, you may use torchmeta for assistance. Set the number of tasks to 16 and use the following CNN model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchmeta.modules import (MetaModule, MetaSequential, MetaConv2d,\n",
    "                               MetaBatchNorm2d, MetaLinear)\n",
    "\n",
    "\n",
    "def conv3x3(in_channels, out_channels, **kwargs):\n",
    "    return MetaSequential(\n",
    "        MetaConv2d(in_channels, out_channels, kernel_size=3, padding=1, **kwargs),\n",
    "        MetaBatchNorm2d(out_channels, momentum=1., track_running_stats=False),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2)\n",
    "    )\n",
    "\n",
    "class ConvolutionalNeuralNetwork(MetaModule):\n",
    "    def __init__(self, in_channels, num_classes, hidden_size=64):\n",
    "        super(ConvolutionalNeuralNetwork, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.features = MetaSequential(\n",
    "            conv3x3(in_channels, hidden_size),\n",
    "            conv3x3(hidden_size, hidden_size),\n",
    "            conv3x3(hidden_size, hidden_size),\n",
    "            conv3x3(hidden_size, hidden_size)\n",
    "        )\n",
    "\n",
    "        self.classifier = MetaLinear(64, num_classes)\n",
    "\n",
    "    def forward(self, inputs,params=None): # load the params of the model optimized on a single task\n",
    "        features = self.features(inputs, params=self.get_subdict(params, 'features'))\n",
    "        features = features.view((features.size(0), -1))\n",
    "        logits = self.classifier(features, params=self.get_subdict(params, 'classifier'))\n",
    "        return logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### A) Meta-Training\n",
    "\n",
    "The meta training consists of two parts. An inner loop which trains the base model using a the support set of a task for a number of epochs and tests it on the query set. In addition to an outer loop which updates the meta-model with the accumalated losses of the query sets based on the number of generated tasks. For simplicity, set the inner epochs to 1. Furthermore, set number of tasks to 16 and outer epochs to 100. Moreover, use an Adam optimizer with learning rate of 0.001 to update the meta-model and use the function *gradient_update_parameters* in torchmeta to update the base model parameters. Finally, during meta-training you should use the train set for training the base model and the test set for calculating the meta-loss and updating the meta-model.\n",
    "\n",
    "\n",
    "**Output**: Plot the accuracy and loss of the meta-model against the number of outer epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from torchmeta.datasets.helpers import cifar_fs,omniglot\n",
    "from torchmeta.utils.data import BatchMetaDataLoader\n",
    "from torchmeta.utils.gradient_based import gradient_update_parameters\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def train_maml(model):\n",
    "    pass\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Meta-testing\n",
    "\n",
    "The testing protocol is to generate a random set of tasks using test set of the dataset. Then we iterate over each task, fine-tune the meta-trained model using the support set and test it on the query set. The process is repeated for each task, eventually the average results should be reported. We will use the same 5-way, 5-shot problem and also set the query set size to 15. Set the task size to 16 and repeat for 100 iterations. You can use *gradient_update_parameters* function to update model parameters also in meta-testing.\n",
    "\n",
    "**Note**: Don't forgot to initilaze the model between each task with the meta-trained parameters to avoid accumalting gradients from previous iterations.\n",
    "\n",
    "**Output** Average test accuracy on Omniglot should be above 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_maml(model):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    model = ConvolutionalNeuralNetwork(1, num_classes=5)\n",
    "\n",
    "    train_maml(model)\n",
    "    test_maml(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Reptile"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reptile uses a different update rule. Hence, you can still rely on your inner and outer loops implemented before in the previous task, however, updating the meta-parameters should be performed by accumlating the difference between the meta-model parameters and the parameters learned on each individual task, as learned in the lecture. In this case we need to modify only the update part. Note that reptile does not require a query set to update the model parameters, unlike MAML.\n",
    "\n",
    "Update rule $$ \\theta \\leftarrow \\theta + \\frac{\\beta}{n} \\sum_{i=1}^{n}{(\\theta_{i}^\\prime - \\theta)}$$\n",
    "\n",
    "Use the same hyperparameters and optimizers as listed in FOMAML, moreover, set Beta to 1e-1. \n",
    "\n",
    "**Output** Use the same meta-testing process and report test accuracy on test set of omniglot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reptile(model):\n",
    "   pass\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training with reptile, you should perform the meta-testing process using the test set of omniglot. Similarly, set the task size to 16 and repeat for 100 iterations, fine-tune the meta-trained model using the support set and test on the query set. For fine-tuning use an Adam optimizer with learning rate 1e-3. You should notice an average test accuracy above 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reptile(model):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    model = ConvolutionalNeuralNetwork(1, num_classes=5)\n",
    "\n",
    "    train_reptile(model)\n",
    "    \n",
    "    test_reptile(model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to repeat the same experiments with 5-way, 1-shot classification task at test time. How do the results differ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atdl_ss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
