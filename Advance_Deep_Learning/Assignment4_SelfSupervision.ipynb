{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alternatvie text](https://www.doc.zuv.fau.de//M/FAU-Logo/01_FAU_Kernmarke/Web/FAU_Kernmarke_Q_RGB_blue.svg)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Self-Supervised Learning "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture you have learned the main concept of Self-Supervised Learning, that is to derive supervision from proxy / surrogate tasks defined over the data. Over the past years, several proxy tasks have been developed to improve the performance of deep neural networks on downstream tasks. There are two main steps inolved in the self-supervision procedure. First, pre-training the backbone model on the pretext task. Next, the backbone model is finetuned on the downstream task.\n",
    "\n",
    "In this assignment, you'll work on the task of microscopy image cell segmentation. You're required to implement a proxy task called \"Pseudo-label microscopy image cell segmentation\". This proxy task pre-trains a cell segmentation network using pseudo-cell segmentation maps extracted via classical computer vision operations. The below figure illustrates the training procedure."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"pretrain.png\"/></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"finetune.png\"/></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need to install the following libraries:\n",
    "- PyTorch\n",
    "- Torchvision\n",
    "- OpenCV\n",
    "- Matplotlib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you're required to download the microscopy image dataset, which you'll work with throughout this assignment. You can use this [link](https://github.com/unidesigner/groundtruth-drosophila-vnc) for download. The task involved in the dataset is binary cell segmentation, where the cells of interest are mitochondria. The microscopy images could be found in the directory \"stack1/raw/\" , while \"stack1/mitochondria/\" contains the corresponding binary cell segmentation groundtruth.\n",
    "\n",
    "The dataset contains in total 20 images of resolution 1024x1024. It would be difficult to train on full resolution images, due to the high resolution. Alternatively, this issue could be solve by training with image crops. Note, the groundtruth should also be cropped at the same crop window location. The below class implements RandomCrop. Use a crop size of 256x256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from PIL import Image\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomCrop(object):\n",
    "\n",
    "    def __init__(self, size, padding=None, pad_if_needed=False, fill=0, padding_mode='constant'):\n",
    "\n",
    "        self.size = size\n",
    "        self.padding = padding\n",
    "        self.pad_if_needed = pad_if_needed\n",
    "        self.fill = fill\n",
    "        self.padding_mode = padding_mode\n",
    "\n",
    "    @staticmethod\n",
    "    def get_params(img, output_size):\n",
    "\n",
    "        w, h = img.squeeze().size()\n",
    "        th, tw = output_size\n",
    "        if w == tw and h == th:\n",
    "            return 0, 0, h, w\n",
    "\n",
    "        i = random.randint(0, h - th)\n",
    "        j = random.randint(0, w - tw)\n",
    "        return i, j, th, tw\n",
    "\n",
    "    def __call__(self, data):\n",
    "\n",
    "        img, mask = data[\"image\"], data[\"label\"]\n",
    "        # pad the width if needed\n",
    "        if self.pad_if_needed and img.shape[0] < self.size[1]:\n",
    "            img = F.pad(img, (self.size[1] - img.shape[0], 0), self.fill, self.padding_mode)\n",
    "            mask = F.pad(mask, (self.size[1] - mask.shape[0], 0), self.fill, self.padding_mode)\n",
    "        # pad the height if needed\n",
    "        if self.pad_if_needed and img.shape[1] < self.size[0]:\n",
    "            img = F.pad(img, (0, self.size[0] - img.shape[1]), self.fill, self.padding_mode)\n",
    "            mask = F.pad(mask, (0, self.size[0] - mask.shape[1]), self.fill, self.padding_mode)\n",
    "        done = False\n",
    "        while not done:\n",
    "            i, j, h, w = self.get_params(img, self.size)\n",
    "            crop_image = F.crop(img, i, j, h, w)\n",
    "            crop_mask = F.crop(mask, i, j, h, w)\n",
    "            if (crop_mask==1).sum(dim=0).sum().item() > int(0.1*h*w):\n",
    "                return crop_image,crop_mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will have to implement your own custom dataset to access the images during training and testing. The dataset will be split into 14 images for training and the rest for testing. You can pick the first 14 images for training, while use the rest for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __len__(self):\n",
    "        pass\n",
    "    def __getitem__(self):\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualize Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize some of the microscopy data i.e. image and groundtruth without cropping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Supervised Learning (Learning from scratch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you will train a fully convolutional regression network (FCRN) with randomly initialized weights and full supervision. Moreover, you should test the trained model on the test set. Use an Adam optimizer with 0.001 learning rate, train for 100 epochs and set the batch size to 14. Furthemore, you should use nn.BCEwithLogitsLoss(). Note since the number of pixels annotated as a foreground is much less than the background you should use weighted BCEloss i.e. nn.BCEwithLogitsLoss(weight=calc_weights(labels)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_bn_relu(in_channels, out_channels, kernel_size,affine=False):\n",
    "    layer = []\n",
    "    layer.append(nn.Conv2d(in_channels, out_channels, kernel_size, padding=1, bias=False))\n",
    "    layer.append(nn.BatchNorm2d(out_channels,affine=affine))\n",
    "    layer.append(nn.ReLU(inplace=True))\n",
    "    return nn.Sequential(*layer)\n",
    "\n",
    "\n",
    "def conv_bn_relu_transpose(in_channels, out_channels, kernel_size,affine=False):\n",
    "    layer = []\n",
    "    layer.append(nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=2, bias=False))\n",
    "    layer.append(nn.BatchNorm2d(out_channels,affine=affine))\n",
    "    layer.append(nn.ReLU(inplace=True))\n",
    "    return nn.Sequential(*layer)\n",
    "\n",
    "class FCRN(nn.Module):\n",
    "    def __init__(self,in_channels=1, out_channels=32, kernel_size=3,affine=True):\n",
    "\n",
    "        super(FCRN, self).__init__()\n",
    "        # Encoder\n",
    "        self.conv1 = conv_bn_relu(in_channels, out_channels, kernel_size,affine=affine)\n",
    "        self.conv2 = conv_bn_relu(out_channels, out_channels * 2, kernel_size,affine=affine)\n",
    "        self.conv3 = conv_bn_relu(out_channels * 2, out_channels * 4, kernel_size,affine=affine)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # LatentSpace\n",
    "        self.conv4 = conv_bn_relu(out_channels * 4, out_channels * 16, kernel_size,affine=affine)\n",
    "\n",
    "        # Decoder\n",
    "        self.conv5 = conv_bn_relu_transpose(out_channels * 16, out_channels * 4, 2,affine=affine)\n",
    "        self.conv6 = conv_bn_relu_transpose(out_channels * 4, out_channels * 2, 2,affine=affine)\n",
    "        self.conv7 = conv_bn_relu_transpose(out_channels * 2, out_channels, 2,affine=affine)\n",
    "        self.conv8 = nn.Conv2d(out_channels, in_channels, 3, padding=1)\n",
    "\n",
    "\n",
    "        self._initialize_weights()\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(self.conv1(x))\n",
    "        x = self.maxpool(self.conv2(x))\n",
    "        x = self.maxpool(self.conv3(x))\n",
    "\n",
    "        x = self.conv4(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.conv7(x)\n",
    "        out = self.conv8(x)\n",
    "        return out\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            if isinstance(m,nn.ConvTranspose2d):\n",
    "                init.normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                if m.affine:\n",
    "                    init.constant_(m.weight, 0.1)\n",
    "                    init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_weights(labels):\n",
    "    pos_tensor = torch.ones_like(labels)\n",
    "\n",
    "    for label_idx in range(0, labels.size(0)):\n",
    "        pos_weight = torch.sum(labels[label_idx] == 1)\n",
    "        neg_weight = torch.sum(labels[label_idx] == 0)\n",
    "        ratio = float(neg_weight.item() / pos_weight.item())\n",
    "        pos_tensor[label_idx] = ratio * pos_tensor[label_idx]\n",
    "\n",
    "    return pos_tensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the segmentation performance, we will use the below function which calculate the mean intersection over union. For more information about the metric you can refer to this [blog](https://hasty.ai/docs/mp-wiki/metrics/iou-intersection-over-union#:~:text=To%20define%20the%20term%2C%20in,matches%20the%20ground%20truth%20data.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(tensor, labels, device=torch.device(\"cpu\")):\n",
    "    iou = 0\n",
    "    foreground_acc = 0\n",
    "\n",
    "    labels_tens = labels.type(torch.BoolTensor)\n",
    "    ones_tens = torch.ones_like(tensor, device=device)\n",
    "    zeros_tens = torch.zeros_like(tensor, device=device)\n",
    "    if tensor.shape[0] > 1:\n",
    "        temp_tens = torch.where(tensor >= 0.5, ones_tens, zeros_tens)\n",
    "        intersection_tens = (temp_tens.squeeze().type(torch.BoolTensor) & labels_tens.squeeze()).float().sum((1, 2))\n",
    "\n",
    "        union_tens = (temp_tens.squeeze().type(torch.BoolTensor) | labels_tens.squeeze()).float().sum((1, 2))\n",
    "        iou += torch.mean((intersection_tens + 0.0001) / (union_tens + 0.0001))\n",
    "        foreground_acc += intersection_tens\n",
    "    else:\n",
    "        temp_tens = torch.where(tensor >= 0.5, ones_tens, zeros_tens)\n",
    "        intersection_tens = (temp_tens.squeeze().type(torch.BoolTensor) & labels_tens.squeeze()).float().sum()\n",
    "        union_tens = (temp_tens.squeeze().type(torch.BoolTensor) | labels_tens.squeeze()).float().sum()\n",
    "        iou += torch.sum((intersection_tens + 0.0001) / (union_tens + 0.0001))\n",
    "        foreground_acc += intersection_tens\n",
    "\n",
    "    total_iou = iou\n",
    "    return total_iou"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the train loss and the train mIoU. You should get a test mIoU of about 45%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "## Implement here your training and test functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Self-Supervised Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you will implement a function which extracts pseudo-cell segmentation maps to train your model on the pretext task. You will use the images of the training set to extract pseudo-labels and you should use the pseudo-labels as your target when calculating the loss. Modify your custom dataset class to extract pseudo-labels during pre-training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract pseudo-labels, you can use OpenCV and follow these steps:\n",
    "- Convert Image to GrayScale cv2.cvtColor().\n",
    "- Apply histogram equalization i.e. cv2.equalizeHist().\n",
    "- Apply binary inversion threshold i.e. cv2.THRESH_BIN_INV, use threshold range of [140, 170].\n",
    "- Convert all values greater than 0 to 255 and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pseudo_labels():\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, you will pre-train your model on the pseudo-label cell segmentation learning with same optimizer, batch, and loss function. However, pretrain only for 50 epochs. Then you will fine-tune the entire pretrained model (encoder and decoder) for 100 epochs assuming you have now access to the groundtruth of the data. You should notice an improvement in the test mIoU of about 10%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# First pretrain\n",
    "\n",
    "# Fine tune entire model\n",
    "\n",
    "# test model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atdl_ss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
